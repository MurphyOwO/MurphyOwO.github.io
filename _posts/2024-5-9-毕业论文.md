# 基于领域知识的问答系统的设计和实现 

##### 摘要

##### 0.引言

让机器阅读文本，理解文本语义，是实现自然语言理解的重要一步。具有极其重要的研究意义，这也是近年来机器阅读理解任务备受关注的原因。机器阅读理解是综合性问题其依赖于分词，词性标注，命名实体识别和依存关系等自然语言处理的基础工作，又可以与机器翻译，情感识别以及信息抽取等技术思想相互借鉴，具有广阔的应用前景。

经过端到端(End-to-End)训练的系统在文本任务上取得了较好的结果，。关键因素之一是在机器阅读理解(Machine Comprehension,MC)中，注意力(Attention)机制被用于聚焦上下文(Context)中与问题(Query)最相关的一段，从而实现一种信息提取的作用。通常工作中的Attention机制具有以下一个或多个特征：1.计算出的注意力权重通常用于从上下文中提取最相关的信息，通过将上下文汇总成固定大小的向量来回答Query；2.对文本而言，它们通常是时间动态的，即当前时间步的注意力权重是根据上一时刻的向量进行计算的；3.它们通常是单向的，即用Query来关注Context[Match LSTM]。

基于知识图谱的问答系统，通过对用户输入问题(Query)进行语义理解，生成结构化查询语句，从给定知识库中选择若干实体或属性值作为该问题的答案。当前知识图谱问答系统在简单句(单实体单属性)上已经取得了比较好的结果，而在约束句：条件约束句，时间约束句吗，以及推理型问句：比较句，最值句，是否型问句以及问句中带有交集，并集和取反的问句等，其逻辑能力还有待提升。在本篇论文中，我们使用双向注意力流网络(Bi-Directional Attention Flow,BIDAF)对我们的问答系统进行设计，从而能够提升我们系统的推理能力。

##### 1.相关工作

之前的端到端的MC研究(或者是问答系统)以三种不同的方式使用注意力机制。第一组(很大程度受到Bahdanau.2015的启发)使用动态注意力机制，其中注意权值在给定查询和上下文的情况下动态更新；Hermann et al. (2015)认为在特定数据集上，动态注意力模型比单个固定的查询向量来关注上下文单词更好；Chen et al. (2016)证明在同一模型中双线性计算注意力权重时可以大大提高精度；Wang & Jiang (2016) 在SQuDA任务上反转注意力方向(随着上下文RNN的进展而查询关注查询词)。与这些模型相比，BiDAF使用了一种不需要记忆的注意力模型。第二组计算一次注意力权重，然后将其输入输出层进行最终预测(e.g., Kadlec et al. (2016)) ；Attention-over-attention模型(Cui et al.， 2016)使用查询和上下文单词之间的二维相似矩阵来计算查询到上下文注意力的加权平均值。BiDAF没在注意力层中使用这两种模式，而是让注意力向量流入建模层。第三组(被认为是记忆网络(Weston et al.， 2015)的变体)通过多层重复计算查询和上下文之间的注意向量，通常被引用作为多跳(Sordoni et al., 2016;  Dhingra et al., 2016)，Shen et al. (2016)将记忆网络和强化学习结合起来控制跳数。这可以作为BiDAF模型的扩展。

知识图谱问答系统(Knowledge Graph Question Answering System，KGQA)是一种基于知识图谱的问答系统，KGQA在回答问题时，将知识图谱的结构和语义信息作为辅助进行推理和答案生成，不同的问答系统在处理知识图谱和注意力机制等方面有所差异。在处理知识图谱上，TransEQA是一种基于知识图谱嵌入的问答系统，它将知识图谱表示为低维向量，然后使用向量相似性的方法来回答问题[TransEQA: Uncovering and Interpreting Coreference Resolution Mechanisms for Machine Reading.]；KG-QA-SRL是一种基于语义角色标注(Semantic Role Labeling，SRL)的知识图谱问答系统，它使用SRL来解析问题中的语义角色，然后将问题和知识图谱的实体进行对齐，最终生成答案。[KG-QA-SRL: Answering Complex Questions by Combining Knowledge Graphs, Natural Language Inference, and Semantic Role Labeling.]；CompGCN是一种基于图卷积神经网络(GCN)的知识图谱问答系统，它使用GCN来对知识图谱进行编码，并使用多头注意力机制来对问题和图谱进行对齐，最终生成答案。[Composition-based Multi-Relational Graph Convolutional Networks for KG Completion and Question Answering.]在注意力机制方面，Match-LSTM是一种基于LSTM网络的问答模型，能够在给定的问题和文本中定位答案。Match-LSTM模型在LSTM中使用了一些注意力机制，以便于提取关键信息；R-Net是一种基于双向递归神经网络（BIRNN）的问答模型，使用了多层BIRNN网络对问题和文本进行编码，然后使用一个基于门控的指针网络，定位答案位置；AoA Reader是一种基于注意力机制的问答模型，它使用多头自注意力机制（Multi-Head Self-Attention）来对问题和文本进行编码，以及对答案位置进行定位。QANet是一种使用卷积神经网络（CNN）和自注意力机制的问答模型。QANet模型可以使用局部自注意力机制来编码问题和文本，使用全局自注意力机制来定位答案位置。

BiDAF运用双向注意力机制将文本中的信息进行提取和理解，然后结合知识图谱的实体和关系信息，进一步推理和生成答案，能够更加准确的去回答问题。

##### 2.模型介绍

我们系统运用的模型是一个分层的多阶段的过程，其由六部分组成。

1. 字符嵌入层(**Character Embedding Layer**)使用字符级别的CNNs将每一个字符映射为一个向量；
2. 词嵌入层(**Word Embedding Layer**)使用预训练词库将每一个词映射为一个向量；
3. 上下文嵌入层(**Contextual Embedding Layer**)利用词上下文的记录来改进词嵌入；
4. 注意力流层(**Attention Flow Layer**)结合问题和上下文向量为上下文中的每个词生成一组与查询相关的特征向量；
5. 建模层(**Modeling Layer**)利用循环神经网络RNN[]去扫面上下文；
6. 输出层(**Output Layer**)生成与问题对应的答案；

我们将前三层记为嵌入层(**Embedding Layers**)，第四，五层记为注意力与建模层(**Attention and Modeling Layers**)，将第六层记为输出层(**Output Layer**)。

![model](C:\Users\dell\Desktop\毕设图片\model.png)

嵌入层使用GloVe作为预训练的词向量，来表示Query和Context中的词；有时候我们会遇见不在词表中的词，则会随机分配一个向量，如果我们在算法中忽略这一点则会影响模型的效果，于是我们需要用另外的嵌入机制来处理，字符嵌入使用一维的卷积神经网络(CNN)来取表达词的含义；此时我们就获取了两种粒度的向量，维度为$d$，$d_1$表示词嵌入(Word Embedding)，$d_2$表示字符嵌入然后累加$d_1$,$d_2$，我们把这个累加称为Highway Network[]，从而去调整词嵌入和字符嵌入的权重关系；能够在处理词方面有更好的效果；最后输出结果就是考虑了词嵌入和字符嵌入后调整出来的向量表示。我们不能仅仅依靠词嵌入和字符嵌入，一词多义会分到同一个向量，这样就需要一个额外的嵌入(Embedding)来理解这个上下文中的词，我们把其称为上下文嵌入层(Contextual Embedding Layer)；LSTM[]能够对每一个时间步的数据关系进行表达，所以我们使用LSTM输出来表示上下文的关系，对于上下文和问题的矩阵我们用$H$(Context matrix)和$U$(Query martix)来表示。到此为止，上下文和问题原始文本已经得到了不同粒度的表征(字符，词，段落)。

注意力机制(Attention)如果我们浅显理解的话，其跟它名字非常相似，核心逻辑就是从关注全部到关注重点。在 Attention 机制引入之前，有一个问题大家一直很苦恼：长距离的信息会被弱化，就好像记忆能力弱的人，记不住过去的事情是一样的。Attention 是挑重点，就算文本比较长，也能从中间抓住重点，不丢失重要的信息。更简单来说就是赋予信息不同的权值。其是解决seq2seq问题最好的办法。下图是一个简单的机器翻译中的Attention示例图：

![img](https://camo.githubusercontent.com/e30d7e346c11d0264238a39a5e8e5ec431642510f5b8cdd654dd2e56eec35cbb/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313230302f312a3156323231444f3951496166683468746b77564259772e6a706567)

BiDAF中的注意力机制与Seq2Seq2中的注意力机制是类似的：

在Decoder中，每一个时间步计算都需要与全部的Encoder Hidden States进行比较，这个过程可以通过任何的函数来将这两个输出进行比较，比如最简单的点乘Dot，这个操作之后的数值被称为Attention Score

我们使用Softmax去计算所有的分值，最后输出一个概率分布，这个分布被称为Attention Distribution，这里获取的是一个跟输入序列相关的分布。图上是一个法语转换成英语的例子,其中蓝色柱形条就是Attention Distribution。在BiDAF中，差异出现在如何计算分值上。

最后获取我们注意力的输出。

BiDAF的核心之处就在于其运用双向注意力流得到了一个问题感知得上下文表征，即Context-to-Query 与 Query-to-Context。注意力流层不用将上下文和问题表示为单个特征向量，而是在每一个步骤的注意力向量不仅来自前一层的嵌入，而且都被允许流入后续的建模层，这样可以减少早期加权和的损失。更深层次的讲，问题感知的上下文表征是所给上下文和问题之间的交互，可以理解为将问题嵌入到上下文中去，这样就给出了一种全新建模上下文和问题的方式。该层的输入是嵌入层得到的H和U，将会得到具有问题感知的上下文表征G。首先我们需要计算H和U的相似度矩阵$S \in \mathbb R^{T \times J}$，$S_{tj}$表示的是上下文H的第t列向量h和问题U中第j列向量u的相似度值(也可以叫做相关性)，其是一个实数值。


$$
S_{tj} = \alpha(H_t,U_{ij}) \in \mathbb R^{T \times J} \\
S_{tj} = \alpha(H_t,U_{ij}) = \alpha(h,u)=W^{T}_{(S)}[h;u;h \circ u]
 
\\ \alpha(a,b) = w^T \times [a;b;a \circ b]
$$
$\alpha$指的是可训练的比较函数，$a$和$b$是两个输入向量，$w^{T}$是指训练权重，$\circ$表示的是同位元素对应相乘，$;$表示向量拼接。得到$S$后将其作为共享相似矩阵Context-to-Query(C2Q)与 Query-to-Context(Q2C)两个方向的Attention，其中每$i$行表示的是上下文文本中第$i$个词与问题文本中每一个词之间的相似度，第$j$列表示的是问题中第$j$个词与上下文文本中每一个词的相似度。下面是两个方向的注意力的计算方式。

* Context-to-Query Attention(C2Q)计算的是对每一个上下文词而言哪些问题词和它最相关

  * $$
    a_{t} = softmax(S_{t:})\in \mathbb R^J
    \\ \tilde{U_{:t}}=\sum_{j}a_{tj}U_{:j}
    $$

  * 将$S$相似度矩阵每一行经过Softmax层直接作为注意力值，$a_t \in \mathbb R^{1 \times T}$，其反映每个问题词对第$t$个上下文词的相对重要性。因为$S$中每一行表示的是上下文文本中第i个词与问题文本中每一个词之间的相似度，C2Q表示的是文本对问题的影响，所以得到$a_t$就直接与$U$中的每一列加权求和得到新的$\tilde{U_{:t}}$，最后拼成新的问题编码$\tilde{U} \in \mathbb R^{2d \times T}$。$\tilde{U}$和$H$相似，是上下文的矩阵表达，但是表达的是不同的信息，$H$表达的是语义，语法和上下文的含义，$\tilde{U}$表达的是每一个问题词与上下文词的相关性。

* Query-to-Context Attention(Q2C)计算的是对每一个问题词而言哪些上下文词和它最相关

  * $$
    b = softmax(max_{col}(S)) \in \mathbb R^T
    \\ \tilde{h} = \sum_tb_tH_{:t} \in \mathbb R^{2d \times 1}
    $$

  * 在这一步中，我们的目标是寻找那个上下文词与问题词最相关，因此这些上下文对回答问题很重要，故我们直接取相关性最大的那一列，对其进行Softmax归一化计算上下文向量加权和，然后重复$T$次得到$\tilde{H}\in \mathbb R^{2d\times T}$，其还包含了一些其他信息，包含了上下文中非常重要的词，考虑问题后上下文中重要的词信息。

* 得到了$\tilde{U}$和$\tilde{H}$两个注意力方向的新问题编码和上下文编码之后，再经过一个MLP函数$\beta$将两者拼接起来得到问题感知的上下文文本表示$G$，即$G_{:t}=\beta(H_{:t},\tilde{U_{t}},\tilde{H_{t}})$，于是我们综合了得到的信息，得到了问题和上下文信息的综合表示$G$。

接下来就是两层bi-LSTM构成的建模层，其输入为$G$。第一个bi-LSTM层将$G$转换为$M_1 \in \mathbb R^{2d \times T}$，然后$M_1$充当第二个bi-LSTM层的输入，第二个bi-LSTM层将其转换为$M_2 \in \mathbb R^{2d \times T}$。$M_1$和$M_2$是上下文词的另一种矩阵表示形式，其与上下文词之前表示形式之间的区别是$M_1$和$M_2$已经嵌入了有关整个上下文段落以及问题信息，这意味着我们已经得到决定谁应该在答案中的所有信息。

最后的输出层就是将我们得到的向量转换为两个概率值，以便我们比较所有上下文词和问题的相关性，用两个分类器预测开始位置以及结束位置。首先将$M_1,M_2,G$进行垂直串联形成$[G;M_1],[G;M_2]$。

通过以下步骤得到$P_1，P_2$，即整个上下文中起始索引的概率分布以及结束索引的概率分布

* $$
  P_1=Softmax(w^{T}_{(p_1)}[G;M_1])
  \\ P_2=Softmax(w^T_{(p_2)}[G;M_2])
  $$

然后使用$P_1,P_2$去寻找答案，这是面对抽取式问答的方法，BiDAF网络允许你根据特定任务修改输出层，但是需要保留其他架构，对于其他问答系统我们需要稍作调整。

##### 3.系统设计

本系统以电信运营商为例，比如比如：“不含彩铃的套餐有哪些？”、“支持长途漫游，价格低于100元的套餐有哪些？”、“神州行B套餐是5G套餐吗”等，这类需要推理的Query目前的问答系统难以回答。本系统运用BiDAF模型与构建的知识库一起“推理”出答案。

* 从训练集出发

  * ![image-20230404113516408](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230404113516408.png)

  * 依据“用知识库推理出问题的答案”这一解题方案，通过简单分析训练集，我们可以发现通过“属性名+实体”在“约束属性值”的学术下推理出答案。如上图“9元百度专属定向流量包如何取消”的属性名是“取消方式”，实体是“专属定向流量包”，约束属性值是“9元”和“百度”，于是我们就可以通过这些信息推出答案为“取消方式_68”。因此，我们可以构建以下的知识库

    * ![image-20230404114356809](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230404114356809.png)
    * ![image-20230404114658943](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230404114658943.png)

    在使用时，我们只需要分析出用户问题的属性(attr)，实体(entity)和约束(constraint)就能够从知识库中推理出问题的答案(ans)。

* 数据分析

  * scheme文件
    * ![image-20230404115226500](C:\Users\dell\Desktop\image-20230404115226500.png)
    * 从上图中可知，训练集中的“属性名”与“约束属性名”的来源是scheme中的“一级属性”和“二级属性”。于是我们知道用户问题的属性名的取值(或约束属性名的取值)范围是确定的，我们可以用分类的方式得到属性名的取值。
    
  * triples文件

    * ![image-20230404135855174](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230404135855174.png)
    * 通过将triples文件中的数据与前面构造的知识库对，我们可以发现triples文件中的第一列为实体，第二列为属性名/约束属性名，第三列为答案/约束属性值。如上图，对"139邮箱_0"，我们可以构造如下知识库。
      * ![image-20230404141253148](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230404141253148.png)

  * synonyms文件

    * ![image-20230404141524870](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230404141524870.png)
    * synonyms文件就是定义了实体的同义词或者用户容易打错的错别字。例如“北京移动plus会员”和“北京移动会员”是同一个含义的实体，“采铃”是用户打错字的“彩铃”。

  * 数据属性名和答案属性名

    数据属性名                                                                           答案属性名

    ├── "上线时间",                                                                  ├── "属性值",

    ├── "业务简介",																  ├── "并列句",

    ├── "价格",																		  ├── "比较句"

    ├── "使用方法",

    ├── "使用范围",

    ├── "内含其它服务",

    ├── "取消方式",

    ├── "取消条件",

    ├── "叠加规则",

    ├── "封顶规则",

    ├── "带宽",

    ├── "开通方式",

    ├── "开通条件",

    ├── "有效期规则",

    ├── "流量",

    ├── "生效规则",

    ├── "结转规则",

    ├── "语音时长",

    ├── "适用app"
    
    * 最后的推理过程我们就是要分别对这18个数据属性名和3答案属性名进行打分，根据分数最高的答案属性名去选择数据属性名从而去在知识库中生成最后的答案。

* 模型设计

  * 在上文介绍中得知BiDAF模型在SQuDA[]任务上取得了极好的效果，经过我们简单分析，以及结合我们的数据类型，我们发现可以将“用户问题”当作SQuDA问题中的“文本”，“答案类型”/“属性名”当作SQuDA中的问题。在SQuDA中，BiDAF通过两个注意力流很好地学到了文本和问题相关的信息，从而找到了问题的答案。在我们的任务中，我们也可以通过两个注意力流来学习“用户答案”与“答案类型”/“属性名”的关系，从而判断该“用户问题”是否与该“答案类型”/"属性名"对应。
  * 在”数据分析-scheme“小节中，我们提到通过分类的方式得出”属性名“。但是直接做多分类是比较困难的，而在简单分析之后我们发现，类属标签(属性名)跟文本(用户问题)是强相关的。例如”9元百度专属定向流量包如何取消“其与”取消方式“有关，而与”开通方式“无关，因此我们可以将其转化为”9元百度专属定向流量包如何取消-取消方式“的分类结果为”pos“，而”9元百度专属定向流量包如何取消-开通方式“为”neg“的一个二分类任务，同时BiDAF模型还能够很好地学习到用户问题和属性名之间的关系。
  * 答案类型也需要处理，这是因为从训练集中我们可以发现答案类型有”属性值“，”并列句“和”比较句“。并列句会有多个”属性值“，例如”你告诉我7天5g视频会员流量包怎么开通，多少钱“会有两个属性名(”开通方式“和”价格“)，在我们推理出答案类型为”并列句后“，后面推理属性名时需要保留两个得分最高的作为“目标属性名”。同时比较句也需要特殊处理，例如”半年包和年包的神州行5元月卡开通方式一样不“的属性名也是”开通方式“，但是其最终结果与答案类型为”属性值“的最终结果是完全不一样的。通过简单分析发现，”并列句“的用户问题中通常会有”，“，而“比较句”的用户问题中通常会有”一样“，”吗“等词出现，在进行特殊处理后，BiDAF可以学习到其关系以提升分类的效果。
  * 与用BiDAF做SQuDA任务(从原文中提取问题的答案)不同，我们需要将输出层(Output Layer)稍微做调整以让其适应我们的二分类任务，因此我们首先需要建模层的输出进行归一然后再用全连接(Full Connext)完成后续的分类，最后在创建的知识库的帮助下”推理“出最终的答案，实际中我们使用的是较弱的实体抽取方式。

* 系统结构

  ├──dataset

  |    ├──knowledges

  |        ├──ans_tree_for_compare.json

  |        ├──ans_tree_from_train_set.json

  |        ├──ans_tree_from_triples.json

  |        ├──ans_type_candidates.json

  |        ├──attr_candidates.json

  |    ├──origin

  |    ├──post

  |    ├── vocab

  |        ├──vocab.txt

  |    ├──origin

  |        ├──scheme.xlsx

  |        ├──synonyms.txt

  |        ├──text1.xlsx

  |        ├──train.xlsx

  |        ├── triples.txt

  ├──results

  |    ├──final_results

  |        ├──kbqa_results.json

  |        ├──result.csv

  |    ├──infer_results

  |        ├──ans_type_result.json

  |        ├──attr_result.json

  |    ├──post_results

  |        ├──ans_from_train_tree.json

  |        ├──ans_from_train_triples_tree.json

  |        ├──ans_processed_comp.json

  |        ├──ans_type_result.json
  |        ├── attr_result.json

  |        ├── entity_result.json

  ├──requirements.txt

  ├── bidaf.py

  ├── config.py

  ├── data_processor.py

  ├── generate_kg.py

  ├── generate_qa_pair.py

  ├── generate_vocab.py

  ├── main.py

  ├── path_env.py

  ├── postprocessor.py

  └── run_model.py

  * knowledges文件夹里包含了对训练集的知识库的构造。

  * path_env中给出了文件的路径信息，包括原始数据文件路径，生成的中间结果路径和最终结果路径。
  * config中包含了BiDAF模型以及训练所需要的参数。
  * generate_kg用于生成知识图谱(知识库)，里面包含用训练集生成的知识库，用triples生成的知识库，和为比较句生成的知识库。
  * generate_qa_pair为将”用户问题“与”答案类型“/”属性名“处理成BiDAF的问题-文本输入模式。
  * generate_vocab为创建训练的词典，用于将输入转换为idx用于模型编码。
  * data_processor为数据处理，将输入数据转化为模型输入格式(batch、padding、文字转idx等)。
  * bidaf就是修改过后的模型。
  * run_model用于串联模型训练各环节，以及推理各环节。
  * postprocessor用于将模型infer结果在创建的知识库的帮助下“推理”出最终的答案。里面包含基于匹配的实体提取，以及逐层推理答案和最终格式化输出的代码逻辑。
  * main为主函数，用于串联所有以达到端到端完成”模型训练+模型推理+答案推理“的整个过程。
  * results文件里包含三个文件，分别表示对测试集的知识图谱的构造结构，问题属性名分类以及答案属性名分类结构，和最终的”推理“出来的答案。

##### 4.实验

配置参数：

```python
import torch

class Config:
    def __init__(self):
        self.vocab_size = 3000
        self.embed_size = 32
        self.hidden_size = 32
        self.num_classes = 2

        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.lr = 1e-3
        self.epochs = 20
        self.batch_size = 256
```

本次任务的评价指标包括宏观准确率(Macro Precision,你认为的正样本中，有多少是真的正确的概率)，宏观召回率(Marco Recall,[正样本](https://www.zhihu.com/search?q=正样本&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A"146711298"})中有多少是被找了出来),Averaged F1值(F1分数同时考虑精确率和召回率，让两者同时达到最高，取得平衡)。设$Q$为问题集合，$A_i$为对第$i$个问题给出的答案集合，$G_i$为第$i$个问题的标准答案集合，计算公式如下
$$
MacroPrecision = \frac{1}{|Q|}\sum_{i=1}^{|Q|}{P_i},P_{i}=\frac{|A_i\cap G_i|}{|A_i|}
\\ MacroRecall =\frac{1}{|Q|}\sum_{i=1}^{|Q|}{R_i}，R_i=\frac{|A_i \cap G_i|}{|G_i|}
\\ AveragedF1=\frac{1}{|Q|}\sum_{i = 1}^{|Q|}\frac{2P_iQ_i}{P_i+Q_i}
$$

训练ans_type的数据图如下：

![image-20230407172827016](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230407172827016.png)

训练attr的数据图如下：

![image-20230407174256607](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230407174256607.png)

最后我们的模型在给定的1000个问题下生成出了855个答案，最后的F1值为$87\%$。

部分结果图：

![image-20230407175345176](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230407175345176.png)

##### 5.总结和期望

本系统使用的模型较为简单，整体过程简洁，实验中的数据也能够迅速收敛；但在有两处地方可以进行优化，1.数据建模的时候，使用上文图中第二种知识库的运行效率应该会更高，因为可以在不同情况进行剪枝；2.可以在模型选择方面使用BERT预训练模型，将“用户问题”当作BERT输入的Segment1，“答案类型”/“属性名”当作BERT输入的Segment2，然后做二分类任务，更多样的Attention应该可以达到更好的效果。

##### 6.引用

#### 7 致谢







​	



